import requests
import zipfile
import io
import os
import json
import uuid

import pandas as pd

import boto3
import os
import concurrent.futures
import time

def upload_file_to_s3(file_path, bucket_name, bucket_path, retries=3, delay=5):
    s3 = boto3.client('s3')
    for i in range(retries):
        try:
            with open(file_path, 'rb') as data:
                s3.upload_fileobj(data, bucket_name, bucket_path)
            print(f'Uploaded {file_path} to s3://{bucket_name}/{bucket_path}')
            return  # If the upload was successful, exit the function
        except Exception as exc:
            print(f'An exception occurred: {exc}. Retrying in {delay} seconds...')
            time.sleep(delay)  # Wait for a while before retrying
    print(f'Failed to upload {file_path} after {retries} attempts.')

def batch_upload_files(files, bucket_name):
    print(f"total file count ({len(files)})")

    # Use a ThreadPoolExecutor to upload the files in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(upload_file_to_s3, file_path, bucket_name, bucket_path) for (file_path, bucket_path) in files}
        count = 0
        for future in concurrent.futures.as_completed(futures):
            count += 1
            print(f"upload counter ({count})")
            try:
                future.result()  # get the result of the upload
            except Exception as exc:
                print(f'An exception occurred: {exc}')

def fetch_and_unzip(url, destination):
    # Send a HTTP request to the URL of the zip file
    print("Fetching file")
    response = requests.get(url)
    print("Finished fetching")
    # Check if the request is successful
    if response.status_code == 200:
        print("Successful")
        z = zipfile.ZipFile(io.BytesIO(response.content))
        # Create destination directory if it doesn't exist
        if not os.path.exists(destination):
            os.makedirs(destination)
        # Extract all the contents of zip file in the specified directory
        z.extractall(path=destination)
        print("Unzipped")
    
first_run = True

if __name__ == "__main__":
    cvedb_path = '/tmp/cvedb'
    
    if first_run:
        print("first run")
        # fetch_and_unzip('https://github.com/CVEProject/cvelistV5/archive/refs/heads/main.zip', cvedb_path)
        # print('Downloaded and extracted the zip file successfully')

    delta_path = os.path.join(cvedb_path, 'cvelistV5-main/cves/delta.json')

    # check for delta file and load the json content
    with open(delta_path, 'r') as f:
        delta_contents = f.read()
        print(delta_contents)

        # delta_contents contains json
        delta_json = json.loads(delta_contents)

    # print the json
    print(delta_json)

    # Print the number of changes
    print("Number of changes:", delta_json["numberOfChanges"])

    import pandas as pd
    import glob

    # Initialize an empty DataFrame
    dfs = {}

    if first_run:
        print("building cve list from local db")
        cve_list = []

        # Define the directory and pattern
        directory = os.path.join(cvedb_path, 'cvelistV5-main', 'cves')
        pattern = "[0-9]" * 4  # A year is a 4-digit number

        print(f"directory: {directory}")

        # Find all files that match the pattern
        directories = list(filter(lambda x: os.path.isdir(x), glob.glob(f"{directory}/**")))
        print(f"Found {len(directories)} directories")

        for directory in directories:
            print(f"directory: {directory}")
            files = glob.glob(f"{directory}/**", recursive=True, )

            # Get the file names
            file_names = [{ "file_name": file } for file in files]
            file_names = list(filter(lambda x: not os.path.isdir(x["file_name"]) and x["file_name"].endswith(".json"), file_names))

            # print(f"filenames ({len(file_names)}): {file_names}")

            # Add the file names to the cve_list
            cve_list.extend(file_names)

        # print(cve_list)
    else:
        if "new" not in delta_json or not delta_json["new"]:
            print("No new CVEs.")
            exit()
        cve_list = delta_json["new"]

    for new_cve in cve_list:
        if "file_name" in new_cve:
            cve_path = new_cve["file_name"]
        else:
            github_link = new_cve["githubLink"]

            # Extract the CVE path from the github link
            cve_path = github_link.replace("https://raw.githubusercontent.com/CVEProject/cvelistV5/main/", "")

        # Create the full local path to the CVE file
        local_cve_path = os.path.join(cvedb_path, 'cvelistV5-main', cve_path)
        print(local_cve_path)

        # Load the CVE file
        with open(local_cve_path, 'r') as f:
            cve_contents = f.read()
            cve_json = json.loads(cve_contents)

        # Print the CVE JSON
        # print(cve_json)

        # Extract the required fields from the cve_json
        data = {
            "state": cve_json["cveMetadata"].get("state") if cve_json["cveMetadata"].get("state") else '',
            "cveId": cve_json["cveMetadata"].get("cveId") if cve_json["cveMetadata"].get("cveId") else '',
            "assignerOrgId": cve_json["cveMetadata"].get("assignerOrgId") if cve_json["cveMetadata"].get("assignerOrgId") else '',
            "assignerShortName": cve_json["cveMetadata"].get("assignerShortName") if cve_json["cveMetadata"].get("assignerShortName") else '',
            "datePublished": cve_json["cveMetadata"].get("datePublished") if cve_json["cveMetadata"].get("datePublished") else '',
            "dateUpdated": cve_json["cveMetadata"].get("dateUpdated") if cve_json["cveMetadata"].get("dateUpdated") else '',
            "providerOrdId": cve_json["containers"]["cna"].get("providerMetadata").get("orgId") if cve_json["containers"]["cna"].get("providerMetadata").get("orgId") else '',
            "providerShortName": cve_json["containers"]["cna"].get("providerMetadata").get("shortName") if cve_json["containers"]["cna"].get("providerMetadata").get("shortName") else '',
        }

        if cve_json["containers"]["cna"].get("descriptions"):
            descriptions = []
            for description in cve_json["containers"]["cna"].get("descriptions"):
                descriptions.append(description.get("value"))

            data["descriptions"] = ",".join(descriptions)
        else:
            data["descriptions"] = ""


        if cve_json["containers"]["cna"].get("affected"):
            affected = []
            for affected_item in cve_json["containers"]["cna"].get("affected"):
                affected.append(f"{affected_item.get('vendor')}:{affected_item.get('product')}:{affected_item.get('versions')}")

            data["affected"] = ",".join(affected)
        else:
            data["affected"] = ""
        
        if cve_json["containers"]["cna"].get("references"):
            references = []
            for reference in cve_json["containers"]["cna"].get("references"):
                references.append(reference.get("url"))

            data["references"] = ",".join(references)
        else:
            data["references"] = ""

        if cve_json["containers"]["cna"].get("problemTypes"):
            problem_types = []
            for problem_type in cve_json["containers"]["cna"].get("problemTypes"):
                descriptions = []
                for description in problem_type.get("descriptions"):
                    descriptions.append(description.get("description"))
                problem_types.append(",".join(descriptions))

            data["problemTypes"] = ",".join(problem_types)
        else:
            data["problemTypes"] = ""


# { "dataType": "CVE_RECORD", "dataVersion": "5.0", 
#  "cveMetadata": { "cveId": "CVE-2024-4014", "assignerOrgId": "b15e7b5b-3da4-40ae-a43c-f7aa60e62599", "state": "PUBLISHED", "assignerShortName": "Wordfence", "dateReserved": "2024-04-19T20:04:16.850Z", "datePublished": "2024-04-20T09:38:16.702Z", "dateUpdated": "2024-04-20T09:38:16.702Z" }, "containers": { "cna": { 
#      "providerMetadata": { "orgId": "b15e7b5b-3da4-40ae-a43c-f7aa60e62599", "shortName": "Wordfence", "dateUpdated": "2024-04-20T09:38:16.702Z" }, 
#      "affected": [ { "vendor": "hcaptcha", "product": "hCaptcha for WordPress", "versions": [ { "version": "*", "status": "affected", "lessThanOrEqual": "4.0.0", "versionType": "semver" } ], "defaultStatus": "unaffected" } ], 
#      "descriptions": [ { "lang": "en", "value": "The hCaptcha for WordPress plugin for WordPress is vulnerable to Stored Cross-Site Scripting via the plugin's cf7-hcaptcha shortcode in all versions up to, and including, 4.0.0 due to insufficient input sanitization and output escaping on user supplied attributes. This makes it possible for authenticated attackers, with contributor-level access and above, to inject arbitrary web scripts in pages that will execute whenever a user accesses an injected page." } ], 
#      "references": [ { "url": "https://www.wordfence.com/threat-intel/vulnerabilities/id/5ce70e87-6dee-4d4a-b2fc-93fd4d50957d?source=cve" }, { "url": "https://plugins.trac.wordpress.org/changeset/3072795/hcaptcha-for-forms-and-more/tags/4.0.1/src/php/CF7/CF7.php" } ], 
#      "problemTypes": [ { "descriptions": [ { "lang": "en", "description": "CWE-79 Improper Neutralization of Input During Web Page Generation ('Cross-site Scripting')" } ] } ], "metrics": [ { "cvssV3_1": { "version": "3.1", "vectorString": "CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:L/I:L/A:N", "baseScore": 6.4, "baseSeverity": "MEDIUM" } } ], "credits": [ { "lang": "en", "type": "finder", "value": "haidv35" } ], "timeline": [ { "time": "2024-04-16T00:00:00.000+00:00", "lang": "en", "value": "Vendor Notified" }, { "time": "2024-04-19T00:00:00.000+00:00", "lang": "en", "value": "Disclosed" } ] } } }


        # Extract the year and cve number from the cveId
        year, cve_number = data["cveId"].split("-")[1:]
        # extract the first two digits of the cve_number as the cve_group
        cve_group = cve_number[:2]

        if year not in dfs:
            dfs[year] = {}
        
        if cve_group not in dfs[year]:
            dfs[year][cve_group] = pd.DataFrame(columns=["state", "cveId", "assignerOrgId", "assignerShortName", "datePublished", "dateUpdated", "providerMetadata", "affected", "descriptions", "references", "problemTypes"])

        # Create a new DataFrame from the data and concatenate it with the existing DataFrame
        dfs[year][cve_group] = pd.concat([dfs[year][cve_group], pd.DataFrame([data])], ignore_index=True)

    print(dfs)

    # Save the dataframe to S3
    import boto3

    upload_info = []

    for year, cve_groups in dfs.items():
        for cve_group, df in cve_groups.items():
            # generate a unique/random uuid for the filename
            filename = str(uuid.uuid4())
            
            # Define the S3 path
            s3_path = f"cvedb/by_year_and_cve/year={year}/cve_group={cve_group}/{filename}.parquet"

            # Save the DataFrame to a parquet file locally
            tmpfile = f'/tmp/{filename}.parquet'
            df.to_parquet(tmpfile)

            print(f"Upload info {s3_path} filename {filename} year {year} cve_group {cve_group}")

            upload_info.append((tmpfile, s3_path))


    batch_upload_files(upload_info, 'elasticninja-cvedb')

    # tmpfile = '/tmp/data.parquet'

    # # Save the DataFrame to a parquet file locally
    # df.to_parquet(tmpfile)

    # # Create a boto3 client
    # s3 = boto3.client('s3')

    # # Extract the year and cve number from the cveId
    # year, cve_number = data["cveId"].split("-")[1:]

    # # extract the first two digits of the cve_number as the cve_group
    # cve_group = cve_number[:2]

    # # Define the S3 path
    # s3_path = f"cvedb/by_year_and_cve/year={year}/cve_group={cve_group}/cve_number={cve_number}/data.parquet"

    # # Upload the parquet file to S3
    # with open(tmpfile, 'rb') as data:
    #     s3.upload_fileobj(data, 'elasticninja-cvedb', s3_path)
