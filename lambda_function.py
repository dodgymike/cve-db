import requests
import zipfile
import io
import os
import json
import uuid

import pandas as pd

import boto3
import os
import concurrent.futures
import time

from openai import OpenAI

print(boto3)

def ask_gpt(prompt, api_key=None, model=None, functions=None):
    model = model or "gpt-3.5-turbo"

    my_id = str(uuid.uuid4())

    params = {
        "model": model,
        "messages": [
            {"role": "system", "content": "assistant"},
            {"role": "user", "content": prompt},
        ],
    }

    if functions:
        params['functions'] = functions

    client = OpenAI(
        # This is the default and can be omitted
        api_key=api_key,
    )

    for _ in range(5):
        try:
            completion = client.chat.completions.create(**params)
            # print("========================================")
            # print(f"completion ({json.dumps(completion)})")
            reply = completion.choices[0].message.content
            # reply = completion['choices'][0]['message']['content']
            return reply
        except Exception as e:
            gpt_error = str(e)
            if '429' in gpt_error:
                gpt_error = "429"

            print(f"{my_id}: {gpt_error}")
            print(f"{my_id}: {prompt}")

            time.sleep(4)

    raise Exception(f"Failed to get GPT response after 5 attempts: {gpt_error}")


def upload_file_to_s3(file_path, bucket_name, bucket_path, retries=3, delay=5):
    s3 = boto3.client('s3')
    for i in range(retries):
        try:
            with open(file_path, 'rb') as data:
                s3.upload_fileobj(data, bucket_name, bucket_path)
            print(f'Uploaded {file_path} to s3://{bucket_name}/{bucket_path}')
            return  # If the upload was successful, exit the function
        except Exception as exc:
            print(f'An exception occurred: {exc}. Retrying in {delay} seconds...')
            time.sleep(delay)  # Wait for a while before retrying
    print(f'Failed to upload {file_path} after {retries} attempts.')

def batch_upload_files(files, bucket_name):
    print(f"total file count ({len(files)})")

    # Use a ThreadPoolExecutor to upload the files in parallel
    with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(upload_file_to_s3, file_path, bucket_name, bucket_path) for (file_path, bucket_path) in files}
        count = 0
        for future in concurrent.futures.as_completed(futures):
            count += 1
            print(f"upload counter ({count})")
            try:
                future.result()  # get the result of the upload
            except Exception as exc:
                print(f'An exception occurred: {exc}')

def fetch_and_unzip(url, destination):
    # Send a HTTP request to the URL of the zip file
    print("Fetching file")
    response = requests.get(url)
    print("Finished fetching")
    # Check if the request is successful
    if response.status_code == 200:
        print("Successful")
        z = zipfile.ZipFile(io.BytesIO(response.content))
        # Create destination directory if it doesn't exist
        if not os.path.exists(destination):
            os.makedirs(destination)
        # Extract all the contents of zip file in the specified directory
        z.extractall(path=destination)
        print("Unzipped")
    
def handler(event, context):
    import boto3

    cvedb_path = '/tmp/cvedb'
    
    first_run = True

    if event.get("first_run"):
        first_run = bool(event.get("first_run"))

    # Initialize an empty DataFrame
    dfs = {}

    # print(boto3)

    # Create a DynamoDB resource
    dynamodb = boto3.resource('dynamodb', region_name='eu-west-1')

    # Specify your table
    # Initialize an empty dictionary
    file_hash_data = {}

    table = dynamodb.Table(os.environ['CVE_FILE_HASH_DB'])
    last_evaluated_key = None

    while True:
        print("Scanning the table")

        # Check if there are more items to fetch
        if last_evaluated_key:
            response = table.scan(ExclusiveStartKey=last_evaluated_key)
        else:
            response = table.scan()
        
        # Load the data into the dictionary
        for item in response['Items']:
            file_hash_data[item['cveid']] = item['filehash']

        if 'LastEvaluatedKey' in response:
            # Update the ExclusiveStartKey with the LastEvaluatedKey
            last_evaluated_key = response['LastEvaluatedKey']
        else:
            break

    # print(json.dumps(file_hash_data))

    print(file_hash_data['CVE-2021-41613'])

    # return


    fetch_and_unzip('https://github.com/CVEProject/cvelistV5/archive/refs/heads/main.zip', cvedb_path)
    print('Downloaded and extracted the zip file successfully')

    if first_run:
        print("first run")

    delta_path = os.path.join(cvedb_path, 'cvelistV5-main/cves/delta.json')

    # check for delta file and load the json content
    with open(delta_path, 'r') as f:
        delta_contents = f.read()
        print(delta_contents)

        # delta_contents contains json
        delta_json = json.loads(delta_contents)

    # print the json
    print(delta_json)

    # Print the number of changes
    print("Number of changes:", delta_json["numberOfChanges"])

    import pandas as pd
    import glob


    if first_run:
        print("building cve list from local db")
        cve_list = []

        # Define the directory and pattern
        directory = os.path.join(cvedb_path, 'cvelistV5-main', 'cves')
        pattern = "[0-9]" * 4  # A year is a 4-digit number

        print(f"directory: {directory}")

        # Find all files that match the pattern
        directories = list(filter(lambda x: os.path.isdir(x), glob.glob(f"{directory}/**")))
        print(f"Found {len(directories)} directories")

        for directory in directories:
            print(f"directory: {directory}")
            files = glob.glob(f"{directory}/**", recursive=True, )

            # Get the file names
            file_names = [{ "file_name": file } for file in files]
            file_names = list(filter(lambda x: not os.path.isdir(x["file_name"]) and x["file_name"].endswith(".json"), file_names))

            # print(f"filenames ({len(file_names)}): {file_names}")

            # Add the file names to the cve_list
            cve_list.extend(file_names)

        # print(cve_list)
    else:
        if ("new" not in delta_json or not delta_json["new"]) and ("updated" not in delta_json or not delta_json["updated"]):
            print("No new/updated CVEs.")
            exit()
        
        cve_list = []

        if "new" in delta_json:
            new_cve_list = delta_json["new"]
            cve_list.extend(new_cve_list)

        if "updated" in delta_json:
            updated_cve_list = delta_json["updated"]
            cve_list.extend(updated_cve_list)

    item_updates = []

    for new_cve in cve_list:
        if "file_name" in new_cve:
            cve_path = new_cve["file_name"]
        else:
            github_link = new_cve["githubLink"]

            # Extract the CVE path from the github link
            cve_path = github_link.replace("https://raw.githubusercontent.com/CVEProject/cvelistV5/main/", "")

        # Create the full local path to the CVE file
        local_cve_path = os.path.join(cvedb_path, 'cvelistV5-main', cve_path)
        # print(local_cve_path)

        import hashlib

        file_hash = None

        # Load the CVE file
        with open(local_cve_path, 'r') as f:
            cve_contents = f.read()
            cve_json = json.loads(cve_contents)
            cveid = cve_json["cveMetadata"]["cveId"]

            # Create a new hash object
            hash_object = hashlib.sha256()
            # Update the hash object with the file contents
            hash_object.update(cve_contents.encode())
            # Get the hexdigest of the hash object
            file_hash = hash_object.hexdigest()

            # print(f"File hash: {file_hash}")
            # if cveid in file_hash_data:
            #     print(f"file_hash_data: {file_hash_data[cveid]}")
            # else:
            #     print(f"file_hash_data: {cveid} not found")

            if cveid in file_hash_data and file_hash_data[cveid] == file_hash:
                # print(f"File hash {file_hash} already exists in the database")
                continue
            else:
                item_updates.append({'cveid': cve_json["cveMetadata"]["cveId"], 'filehash': file_hash})

        # Print the CVE JSON
        # print(cve_json)

        # Extract the required fields from the cve_json
        data = {
            "fileHash": file_hash,
            "state": cve_json["cveMetadata"].get("state") if cve_json["cveMetadata"].get("state") else '',
            "cveId": cve_json["cveMetadata"].get("cveId") if cve_json["cveMetadata"].get("cveId") else '',
            "assignerOrgId": cve_json["cveMetadata"].get("assignerOrgId") if cve_json["cveMetadata"].get("assignerOrgId") else '',
            "assignerShortName": cve_json["cveMetadata"].get("assignerShortName") if cve_json["cveMetadata"].get("assignerShortName") else '',
            "datePublished": cve_json["cveMetadata"].get("datePublished") if cve_json["cveMetadata"].get("datePublished") else '',
            "dateUpdated": cve_json["cveMetadata"].get("dateUpdated") if cve_json["cveMetadata"].get("dateUpdated") else '',
            "providerOrdId": cve_json["containers"]["cna"].get("providerMetadata").get("orgId") if cve_json["containers"]["cna"].get("providerMetadata").get("orgId") else '',
            "providerShortName": cve_json["containers"]["cna"].get("providerMetadata").get("shortName") if cve_json["containers"]["cna"].get("providerMetadata").get("shortName") else '',
        }

        # HACK REMOVE ME
        # if data["cveId"] != "CVE-2024-32735":
        #     continue
        # else:
        #     print("Found CVE-2024-32735")

        if cve_json["containers"]["cna"].get("descriptions"):
            descriptions = []
            for description in cve_json["containers"]["cna"].get("descriptions"):
                descriptions.append(description.get("value"))

            data["descriptions"] = ",".join(descriptions)
        else:
            data["descriptions"] = ""


        if cve_json["containers"]["cna"].get("affected"):
            affected = []
            for affected_item in cve_json["containers"]["cna"].get("affected"):
                affected.append(f"{affected_item.get('vendor')}:{affected_item.get('product')}:{affected_item.get('versions')}")

            data["affected"] = ",".join(affected)
        else:
            data["affected"] = ""
        
        if cve_json["containers"]["cna"].get("references"):
            references = []
            for reference in cve_json["containers"]["cna"].get("references"):
                references.append(reference.get("url"))

            data["references"] = ",".join(references)
        else:
            data["references"] = ""

        if cve_json["containers"]["cna"].get("problemTypes"):
            problem_types = []
            for problem_type in cve_json["containers"]["cna"].get("problemTypes"):
                descriptions = []
                for description in problem_type.get("descriptions"):
                    descriptions.append(description.get("description"))
                problem_types.append(",".join(descriptions))

            data["problemTypes"] = ",".join(problem_types)
        else:
            data["problemTypes"] = ""

#         gpt_response = ask_gpt(f"""
#                The following describes a software vulnerability:
# {data["descriptions"]}

# """ + """
# Provide the following IN JSON FORMAT ONLY:
# tldr, affected software,affected versions,vulnerability type

# Example response:
# {
# 'tldr': 'Wordpress is vulnerable to cross-site scripting due to poor input sanitisation',
# 'affected-software': 'PHP on Wordpress',
# 'affected-versions': 'up to and including 4.0.0',
# 'vulnerability-type': 'cross-site-scripting'
# })
# """, api_key=os.environ['OPENAI_API_KEY'])
        
#         gpt_summary = json.loads(gpt_response)
#         print(f"GPT Response: ({json.dumps(gpt_summary)})")

#         raise Exception("Moo")

        # Extract the year and cve number from the cveId
        year, cve_number = data["cveId"].split("-")[1:]
        # extract the first digit of the cve_number as the cve_group
        cve_group = cve_number[:2]

        # extract the first two digits of the cve_number as the cve_group
        # cve_group = cve_number[:2]

        if year not in dfs:
            dfs[year] = {}
        
        if cve_group not in dfs[year]:
            dfs[year][cve_group] = pd.DataFrame(columns=["state", "cveId", "assignerOrgId", "assignerShortName", "datePublished", "dateUpdated", "providerMetadata", "affected", "descriptions", "references", "problemTypes"])

        # Create a new DataFrame from the data and concatenate it with the existing DataFrame
        dfs[year][cve_group] = pd.concat([dfs[year][cve_group], pd.DataFrame([data])], ignore_index=True)

    print(dfs)

    # batch update the table with file hashes
    with table.batch_writer() as batch:
        for item in item_updates:
            batch.put_item(Item=item)

    # # HACK REMOVE ME
    # return

    # Save the dataframe to S3
    import boto3

    upload_info = []

    for year, cve_groups in dfs.items():
        for cve_group, df in cve_groups.items():
            # generate a unique/random uuid for the filename
            filename = str(uuid.uuid4())

            # if int(year) < 2024 or int(cve_group) <= 39:
            #     continue
            
            # Define the S3 path
            s3_path = f"cvedb/by_year_and_cve/year={year}/cve_group={cve_group}/{filename}.parquet"

            # Save the DataFrame to a parquet file locally
            tmpfile = f'/tmp/{filename}.parquet'
            df.to_parquet(tmpfile)

            print(f"Upload info {s3_path} filename {filename} year {year} cve_group {cve_group}")

            upload_info.append((tmpfile, s3_path))


    batch_upload_files(upload_info, 'elasticninja-cvedb')

    # tmpfile = '/tmp/data.parquet'

    # # Save the DataFrame to a parquet file locally
    # df.to_parquet(tmpfile)

    # # Create a boto3 client
    # s3 = boto3.client('s3')

    # # Extract the year and cve number from the cveId
    # year, cve_number = data["cveId"].split("-")[1:]

    # # extract the first two digits of the cve_number as the cve_group
    # cve_group = cve_number[:2]

    # # Define the S3 path
    # s3_path = f"cvedb/by_year_and_cve/year={year}/cve_group={cve_group}/cve_number={cve_number}/data.parquet"

    # # Upload the parquet file to S3
    # with open(tmpfile, 'rb') as data:
    #     s3.upload_fileobj(data, 'elasticninja-cvedb', s3_path)

if __name__ == "__main__":
    import boto3
    print(boto3)
    handler({}, {})
